{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4748193,"sourceType":"datasetVersion","datasetId":2623467}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:17:46.555713Z","iopub.execute_input":"2025-05-25T03:17:46.555902Z","iopub.status.idle":"2025-05-25T03:17:46.821053Z","shell.execute_reply.started":"2025-05-25T03:17:46.555881Z","shell.execute_reply":"2025-05-25T03:17:46.819760Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/meddialogue/test_data.json\n/kaggle/input/meddialogue/english-dev.json\n/kaggle/input/meddialogue/healthcaremagic_dialogue_3.txt\n/kaggle/input/meddialogue/train_data.json\n/kaggle/input/meddialogue/healthcaremagic_dialogue_4.txt\n/kaggle/input/meddialogue/healthcaremagic_dialogue_2.txt\n/kaggle/input/meddialogue/healthcaremagic_dialogue_1.txt\n/kaggle/input/meddialogue/english-train.json\n/kaggle/input/meddialogue/english-test.json\n/kaggle/input/meddialogue/icliniq_dialogue.txt\n/kaggle/input/meddialogue/validate_data.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q torch transformers datasets accelerate peft evaluate rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:17:59.175153Z","iopub.execute_input":"2025-05-25T03:17:59.175382Z","iopub.status.idle":"2025-05-25T03:19:24.146741Z","shell.execute_reply.started":"2025-05-25T03:17:59.175361Z","shell.execute_reply":"2025-05-25T03:19:24.145959Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m97.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m250.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n\n# Replace with your actual dataset folder name\ndataset_path = '/kaggle/input/meddialogue'\n\nprint(\"Files in dataset folder:\")\nprint(os.listdir(dataset_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:19:48.231661Z","iopub.execute_input":"2025-05-25T03:19:48.231927Z","iopub.status.idle":"2025-05-25T03:19:48.239135Z","shell.execute_reply.started":"2025-05-25T03:19:48.231904Z","shell.execute_reply":"2025-05-25T03:19:48.238471Z"}},"outputs":[{"name":"stdout","text":"Files in dataset folder:\n['test_data.json', 'english-dev.json', 'healthcaremagic_dialogue_3.txt', 'train_data.json', 'healthcaremagic_dialogue_4.txt', 'healthcaremagic_dialogue_2.txt', 'healthcaremagic_dialogue_1.txt', 'english-train.json', 'english-test.json', 'icliniq_dialogue.txt', 'validate_data.json']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/input/meddialogue/english-train.json\", \"r\") as f:\n    train_data = json.load(f)\n\nwith open(\"/kaggle/input/meddialogue/english-dev.json\", \"r\") as f:\n    val_data = json.load(f)\n\nwith open(\"/kaggle/input/meddialogue/english-test.json\", \"r\") as f:\n    test_data = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:19:53.743861Z","iopub.execute_input":"2025-05-25T03:19:53.744075Z","iopub.status.idle":"2025-05-25T03:19:53.795943Z","shell.execute_reply.started":"2025-05-25T03:19:53.744061Z","shell.execute_reply":"2025-05-25T03:19:53.795312Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from datasets import Dataset\n\ndef format_dialogue_data(data):\n    formatted = []\n    for item in data:\n        utterances = item.get(\"utterances\", [])\n        if len(utterances) >= 2 and utterances[0].lower().startswith(\"patient\") and utterances[1].lower().startswith(\"doctor\"):\n            question = utterances[0].replace(\"patient:\", \"\").strip()\n            answer = utterances[1].replace(\"doctor:\", \"\").strip()\n            text = f\"Patient: {question}\\nDoctor: {answer} [Disclaimer: This is not medical advice.]\"\n            formatted.append({\"text\": text})\n    return formatted\n\ntrain_samples = format_dialogue_data(train_data)\nval_samples = format_dialogue_data(val_data)\ntest_samples = format_dialogue_data(test_data)\n\ntrain_dataset = Dataset.from_list(train_samples)\nval_dataset = Dataset.from_list(val_samples)\ntest_dataset = Dataset.from_list(test_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:19:56.005106Z","iopub.execute_input":"2025-05-25T03:19:56.005663Z","iopub.status.idle":"2025-05-25T03:19:57.246908Z","shell.execute_reply.started":"2025-05-25T03:19:56.005648Z","shell.execute_reply":"2025-05-25T03:19:57.246317Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# To resume from checkpoint, change this path\nmodel_path = \"gpt2-medium\"  # or \"mediguide_checkpoint_epoch2\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.resize_token_embeddings(len(tokenizer))\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:20:01.369340Z","iopub.execute_input":"2025-05-25T03:20:01.369588Z","iopub.status.idle":"2025-05-25T03:20:32.396966Z","shell.execute_reply.started":"2025-05-25T03:20:01.369570Z","shell.execute_reply":"2025-05-25T03:20:32.396265Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bb6b21d667246cd82180254b8edf229"}},"metadata":{}},{"name":"stderr","text":"2025-05-25 03:20:12.683969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748143212.853020      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748143212.902070      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccda98582f974a049c9bbe284e4ef6dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f210c56d5e242b2a467442d2a4d035e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"421ebf95d72944b1b7f0110243ffbc77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60c2790890a84716a03edeac00eb7a8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9dabca4e7c49caa15177db313e956b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2b0d93752f44713bbc5e6565f55df74"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=3072, nx=1024)\n          (c_proj): Conv1D(nf=1024, nx=1024)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=4096, nx=1024)\n          (c_proj): Conv1D(nf=1024, nx=4096)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\"\n    )\n\ntokenized_train = train_dataset.map(tokenize_function, batched=True)\ntokenized_val = val_dataset.map(tokenize_function, batched=True)\ntokenized_test = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:20:47.742028Z","iopub.execute_input":"2025-05-25T03:20:47.742771Z","iopub.status.idle":"2025-05-25T03:20:48.220291Z","shell.execute_reply.started":"2025-05-25T03:20:47.742755Z","shell.execute_reply":"2025-05-25T03:20:48.219390Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/482 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1336cd1b6b84e55bf6469454db1cb62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a8b33b7d1c64c0291a88582ae813037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/61 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78028d899a3d4e4292abe58cb628d9dc"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\ntrain_loader = DataLoader(tokenized_train, batch_size=2, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:20:50.529629Z","iopub.execute_input":"2025-05-25T03:20:50.530246Z","iopub.status.idle":"2025-05-25T03:20:52.211266Z","shell.execute_reply.started":"2025-05-25T03:20:50.530222Z","shell.execute_reply":"2025-05-25T03:20:52.210548Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom transformers import get_scheduler\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nnum_epochs = 2\nnum_training_steps = len(train_loader) * num_epochs\n\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:20:56.020347Z","iopub.execute_input":"2025-05-25T03:20:56.021007Z","iopub.status.idle":"2025-05-25T03:20:56.048253Z","shell.execute_reply.started":"2025-05-25T03:20:56.020987Z","shell.execute_reply":"2025-05-25T03:20:56.047745Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel.train()\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}\")\n    loop = tqdm(train_loader)\n    for batch in loop:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"input_ids\"]\n        )\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        loop.set_description(f\"Epoch {epoch+1}\")\n        loop.set_postfix(loss=loss.item())\n\n    # ✅ Save after each epoch\n    checkpoint_path = f\"mediguide_checkpoint_epoch{epoch+1}\"\n    model.save_pretrained(checkpoint_path)\n    tokenizer.save_pretrained(checkpoint_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T03:20:57.928689Z","iopub.execute_input":"2025-05-25T03:20:57.928921Z","iopub.status.idle":"2025-05-25T05:40:03.390696Z","shell.execute_reply.started":"2025-05-25T03:20:57.928906Z","shell.execute_reply":"2025-05-25T05:40:03.387646Z"}},"outputs":[{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/241 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nEpoch 1: 100%|██████████| 241/241 [1:09:43<00:00, 17.36s/it, loss=1]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 241/241 [1:09:15<00:00, 17.24s/it, loss=0.739]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ✅ Save model and tokenizer after Epoch 2\nmodel.save_pretrained(\"/kaggle/working/mediguide_checkpoint_epoch2\")\ntokenizer.save_pretrained(\"/kaggle/working/mediguide_checkpoint_epoch2\")\n\n# ✅ Zip the folder so it can be downloaded\n!zip -r /kaggle/working/mediguide_checkpoint_epoch2.zip /kaggle/working/mediguide_checkpoint_epoch2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:45:08.508336Z","iopub.execute_input":"2025-05-25T05:45:08.508677Z","iopub.status.idle":"2025-05-25T05:46:30.079563Z","shell.execute_reply.started":"2025-05-25T05:45:08.508615Z","shell.execute_reply":"2025-05-25T05:46:30.078672Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/mediguide_checkpoint_epoch2/ (stored 0%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/vocab.json (deflated 59%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/tokenizer.json (deflated 82%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/model.safetensors (deflated 7%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/tokenizer_config.json (deflated 54%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/merges.txt (deflated 53%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/config.json (deflated 52%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/special_tokens_map.json (deflated 60%)\n  adding: kaggle/working/mediguide_checkpoint_epoch2/generation_config.json (deflated 24%)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'mediguide_checkpoint_epoch2.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:58:47.413402Z","iopub.execute_input":"2025-05-25T05:58:47.413928Z","iopub.status.idle":"2025-05-25T05:58:47.418005Z","shell.execute_reply.started":"2025-05-25T05:58:47.413910Z","shell.execute_reply":"2025-05-25T05:58:47.417503Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/mediguide_checkpoint_epoch2.zip","text/html":"<a href='mediguide_checkpoint_epoch2.zip' target='_blank'>mediguide_checkpoint_epoch2.zip</a><br>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Load the saved model\nmodel = AutoModelForCausalLM.from_pretrained(\"mediguide_checkpoint_epoch2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mediguide_checkpoint_epoch2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:33:32.873182Z","iopub.execute_input":"2025-05-25T06:33:32.873486Z","iopub.status.idle":"2025-05-25T06:33:33.175407Z","shell.execute_reply.started":"2025-05-25T06:33:32.873468Z","shell.execute_reply":"2025-05-25T06:33:33.174680Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=3072, nx=1024)\n          (c_proj): Conv1D(nf=1024, nx=1024)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=4096, nx=1024)\n          (c_proj): Conv1D(nf=1024, nx=4096)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\n# Load validation data\nwith open(\"/kaggle/input/meddialogue/english-dev.json\", \"r\") as f:\n    val_data = json.load(f)\n\n# Format it into the proper prompt/response format\ndef format_dialogue_data(data):\n    formatted = []\n    for item in data:\n        utterances = item.get(\"utterances\", [])\n        if len(utterances) >= 2 and utterances[0].lower().startswith(\"patient\") and utterances[1].lower().startswith(\"doctor\"):\n            question = utterances[0].replace(\"patient:\", \"\").strip()\n            answer = utterances[1].replace(\"doctor:\", \"\").strip()\n            text = f\"Patient: {question}\\nDoctor:\"\n            formatted.append({\"prompt\": text, \"reference\": answer})\n    return formatted\n\neval_data = format_dialogue_data(val_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:33:43.747977Z","iopub.execute_input":"2025-05-25T06:33:43.748393Z","iopub.status.idle":"2025-05-25T06:33:43.784461Z","shell.execute_reply.started":"2025-05-25T06:33:43.748378Z","shell.execute_reply":"2025-05-25T06:33:43.783809Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"eval_data = eval_data[:100]  # Use 100 samples for ROUGE evaluation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:33:58.478321Z","iopub.execute_input":"2025-05-25T06:33:58.478617Z","iopub.status.idle":"2025-05-25T06:33:58.482443Z","shell.execute_reply.started":"2025-05-25T06:33:58.478575Z","shell.execute_reply":"2025-05-25T06:33:58.481819Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model.eval()\n\ngenerated_texts = []\nreference_texts = []\n\nfor sample in eval_data:\n    inputs = tokenizer(sample[\"prompt\"], return_tensors=\"pt\", truncation=True).to(device)\n\n    # ✅ Use attention_mask + generate only new tokens (e.g., 50 tokens after prompt)\n    output_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=50,  # Only generate up to 50 new tokens after the prompt\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    # Decode and extract the doctor's response\n    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    if \"Doctor:\" in generated:\n        generated_answer = generated.split(\"Doctor:\")[-1].strip()\n    else:\n        generated_answer = generated.strip()\n\n    generated_texts.append(generated_answer)\n    reference_texts.append(sample[\"reference\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:35:16.886324Z","iopub.execute_input":"2025-05-25T06:35:16.886636Z","iopub.status.idle":"2025-05-25T06:40:05.214054Z","shell.execute_reply.started":"2025-05-25T06:35:16.886588Z","shell.execute_reply":"2025-05-25T06:40:05.213330Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from evaluate import load\n\nrouge = load(\"rouge\")\n\nresults = rouge.compute(predictions=generated_texts, references=reference_texts)\n\n# Display results\nprint(\"ROUGE Evaluation Results:\")\nfor key, value in results.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:41:30.547783Z","iopub.execute_input":"2025-05-25T06:41:30.548043Z","iopub.status.idle":"2025-05-25T06:41:31.283898Z","shell.execute_reply.started":"2025-05-25T06:41:30.548027Z","shell.execute_reply":"2025-05-25T06:41:31.283302Z"}},"outputs":[{"name":"stdout","text":"ROUGE Evaluation Results:\nrouge1: 0.1990\nrouge2: 0.0536\nrougeL: 0.1481\nrougeLsum: 0.1480\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\nimport math\n\nmodel.eval()\ntotal_loss = 0\nnum_batches = 0\n\n# You can reduce batch size or sample count if this is slow\nfor sample in eval_data[:100]:  # Evaluate on first 100 samples\n    inputs = tokenizer(\n        sample[\"prompt\"] + sample[\"reference\"],\n        return_tensors=\"pt\",\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\"\n    ).to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n        loss = outputs.loss\n        total_loss += loss.item()\n        num_batches += 1\n\n# Final perplexity\navg_loss = total_loss / num_batches\nperplexity = math.exp(avg_loss)\n\nprint(f\"Average Loss: {avg_loss:.4f}\")\nprint(f\"Perplexity (PPL): {perplexity:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:45:41.038902Z","iopub.execute_input":"2025-05-25T06:45:41.039431Z","iopub.status.idle":"2025-05-25T06:48:24.299999Z","shell.execute_reply.started":"2025-05-25T06:45:41.039417Z","shell.execute_reply":"2025-05-25T06:48:24.299338Z"}},"outputs":[{"name":"stdout","text":"Average Loss: 1.2206\nPerplexity (PPL): 3.39\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}